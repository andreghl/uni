---
title: "Tutorial3 - Nov 16th, 2023"
author: "Andre"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Setting up R

```{R}

# Import the data
load("BEL_data.RData") # Working Directory -> "Empirical Econometrics"\Tutorials

# Return the names of each variable
names(BEL_data)

# Address the variables with their label
attach(BEL_data)
```

## Visual Inspectation of Stationarity *(Recap)*

```{R}

IO_ts <- ts(BEL_data$IO, start = 1950, frequency = 1)
YO_ts <- ts(BEL_data$YO, start = 1950, frequency = 1)

lnIO <- log(IO_ts)
lnYO <- log(YO_ts)

ts.plot(lnIO, lnYO, ylab = "IO and YO", col = c("blue", "black"), lty = c(1,1))
legend("topleft", legend = c("log(IO)", "log(YO)"), col = c("blue", "black"), lty = c(1,1))
title(main= "Time series plot of Investment and Output", sub = "In the logarithmic form", xlab = "Time")

```


**Are these time series likely to be stationary?**

These trends are very unlikely to be stationary due to the obvious upward trend in the data.

``` {R}

# Create new variables (difference of variables)

dlnIO <- diff(log(IO_ts))
dlnYO <- diff(log(YO_ts))

ts.plot(dlnIO, dlnYO, col = c("blue", "black"), lty = c(1,1))
title(main = "Time series plot of Investment and Output", sub = "In the difference and logarithmic form",ylab = "IO and YO", xlab = "Time")
legend("bottomright", legend = c("diff(lnIO)", "diff(lnYO)"), col = c("blue", "black"), lty = c(1,1))

```

**Are these time series likely to be stationary?**

In this case, they are more likely to be stationary. The mean seems close to 0 and the homoskedasticity assumption seems to hold as well.

## 3. Visual Inspectation of Trends

Consider the following regression model for $lnIO_{t}$ with a trend:

$$
\tag{13}
lnIO_{t} = \beta_{0} + \beta_{1}t + u_{t}
$$
```{R}

# Creating a trend variable

n <- length(lnIO)
trend <- 1:n

# Estimate model (13)

fit_lnIO_t <- lm(lnIO~trend)
summary(fit_lnIO_t)
```
The coefficient $\beta_{1}$ show us the average linear trend of the data over our time period.

```{R}

resid_fit_lnIO_t <- fit_lnIO_t$residuals

```

**What do the residuals intuitively represent?**

These residuals shows us the part of the data that is not predicted by our model. In a sense they are the deviations from our model.

``` {R}

# Time plot of the residuals

plot(fit_lnIO_t$residuals, type = "l", ylab = "Residuals", xlab = "Time")
title(main = "Residuals of model (13)")

```

**Make a time series plot of the residual series.**

```{R}

ts.plot(resid_fit_lnIO_t, col = "blue", lty = 1, ylab = "Residuals", xlab = "Time")
title(main = "Time series plot of the Residuals of model (13)")
```


**Definitions**

- Deterministic trend:

A deterministic trend is a trend that can be directly determined from the equation. 

*Example:*

The time series process $y_{t} = ct + \epsilon$ has a deterministic trend with an expected value of $E[y_{t}]$ and a constant variance of $Var(y_{t}) = \sigma^{2}$.

- Stochastic trend:

A stochastic trend is a trend that can change in each round due to the random nature component of the process.

*Example:*

The time series process $y_{t} = c + y_{t - 1} + \epsilon_{t}$ has the same expected value  $E[y_{t}]$ but a non-constant variance of $Var(y_{t}) = t \sigma^{2}$. The random component is generated by $\epsilon_{t}$.

*Source:*
[Cross Validated](https://stats.stackexchange.com/questions/241144/explain-what-is-meant-by-a-deterministic-and-stochastic-trend-in-relation-to-the#:~:text=The%20deterministic%20trend%20is%20one%20that%20you%20can,%CE%B5%20%E2%88%92%20i%20i%20d%20%280%2C%20%CF%83%202%29.)


**Do you think it is likely that $lnIO_{t}$ has a deterministic or a stochastic trend?**

I believe that the variable $lnIO_{t}$ has a deterministic trend since the logarithm is used to transform a given time series into a linear time series (by stabilizing the variance).

**Correction:** Stochastic trend.

**What does it mean for a series to be trend stationary**

This means that once we have removed the trend, the residuals will be a stationary stochastic process.

**Does it then have a deterministic or a stochastic trend?**

It seems that the residuals of model $(13)$ have a deterministic trend since the expected value of the process seems to equal $0$ and there also seems to be a constant variance. 

**Correction:** Stochastic trend.

**Repeat the same process for $lnYO_{t}$**

Consider the following regression model for $lnYO_{t}$ with a trend:

$$
\tag{13'}
lnYO_{t} = \beta_{0} + \beta_{1} t + u_{t}
$$
```{R}

# Estimate model (13')

fit_lnYO_t <- lm(lnYO~trend)
summary(fit_lnYO_t)
```

Again $\beta_{1}$ shows the average linear trend across our sample.

```{R}

resid_fit_lnYO_t <- fit_lnYO_t$residuals
```

Again, they represent the variation in our dependent variable that is not explained by our model.

```{R}

# Residual plot

plot(resid_fit_lnYO_t, type = "l", ylab = "Residual", xlab = "Time")
title(main = "Time series plot of the residuals of model (13')")
```

**Do you think it is likely that $lnYO_{t}$ has a deterministic or a stochastic trend?**

I believe that $lnYO_{t}$ is likely to have a deterministic trend.

**What does it mean for a series to be trend stationary**

This means that once we have removed the trend, the residuals will be a stationary stochastic process.

**Does it then have a deterministic or a stochastic trend?**

It seems that the residuals of model $(13')$ have a deterministic trend since the expected value of the process seems to equal $0$ and there also seems to be a constant variance.

**What is your conclusion: is $lnYO_{t}$ likely to have a deterministic or a stochastic trend?**

The trend of the variable $lnYO_{t}$ seems to be deterministic.

## 4. Dickey-Fuller Unit Root Test (with constant and trend)

To formally test whether a series has a stochastic or a deterministic trend, we need to perform a unit root test with constant and trend.

**What is the null hypothesis of this unit root test? What is the alternative hypothesis?**

The null hypothesis of the Dickey-Fuller test is that a unit root is present in our time series model. The alternative hypos-thesis is that there are none.

**Dickey-Fuller (DF) test for $lnIO_{t}$**

```{R}

# Installing necessary package

library(bootUR)

df_lnIO <- adf(lnIO, deterministics = "trend", max_lag = 0)
df_lnIO
```
The p-value is too high (above the 5% significance level) meaning we cannot reject the null.
There is a high probability that $lnIO_{t}$ has a unit root which is bad for the stationarity assumption.

**What is your conclusion for $lnIO_{t}$: does it have a stochastic or a deterministic trend?**

The test allows us to conclude that $lnIO_{t}$ has a stochastic trend.

**How to proceed in case of a stochastic trend? How to proceed in case of a deterministic trend?**

For a stochastic trend, it might be a good idea to take the first difference of the model and for the deterministic trend, you might want to add the variable $t$ to your model to detrend it.

```{R}

df_lnYO <- adf(lnYO, deterministics = "trend", max_lag = 0)
df_lnYO

```
Again, the p-value is very high thus we cannot reject the null hypothsis. There are some chances that $lnYO_{t}$ has a unit root.

**What is your conclusion for $lnYO_{t}$: does it have a stochastic or a deterministic trend?**

The test allows us to conclude that $lnYO_{t}$ has a stochastic trend.

## 5. Augmented Dickey-Fuller Unit Root Test (with constant and trend)

**How does the ADF unit root test differ from the DF test?** 

The Dickey-Fuller test for the presence of unit root in the time series process and the augmented Dickey-Fuller tests for the same thing but removes the autocorrelation in the sample beforehand.

**Why is the augmentation needed?**

The autocorrelation in the process might result in wrong estimates for the t-test.

```{R}

# Testing the ADF

adf_lnIO <- adf(lnIO, deterministics = "trend")
adf_lnIO
```

This test leads to the same conclusion has the previous one. The variable $lnIO_{t}$ has a stochastic process.

**Repeat the same exercise for $lnYO_{t}$. What do you conclude?**

```{R}

adf_lnYO <- adf(lnYO, deterministics = "trend")
adf_lnYO
```
Again, we can reach the same conclusion as with the standard DF test. The variable $lnYO_{t}$ has a stochastic process.

## 6. Bootstrap union of rejection test

In the previous exercise, we used the ADF test as a unit root test,
which is by far the most popular unit root test. Still the ADF test requires us to specify which deterministic components to include in the test equation (a constant and a trend in case the series displays a trend; a constant only when the series displays no trend). To relieve the user of making this choice (in case it is not so clear cut), you may use the union of rejections test instead. The null hypothesis and alternative hypothesis stay the same as before.

```{R}

# Union of rejection test 

union_lnIO = boot_union(lnIO)
union_lnIO
```

The p-value of this test for the variable $lnIO_{t}$ is bigger than that of the DF and ADF test, thus again, we can conclude that the process has a unit root -> stochastic.

There are no estimate for largest root. -> Why?

**Repeat the same for $lnYO_{t}$. What can you conclude?

```{R}

# Union of rejection test 

union_lnYO <- boot_union(lnYO)
union_lnYO

```

The p-value is slightly lower than the one obtained from the DF and ADF tests. Surprisingly, the program claims that the "estimate largest root" is "NA" which was not the case previously.

The time series process has a unit root and thus is stochastic.

## 7. Unit Root Test on the series in log-differences

The series $lnIO_{t}$ or $lnYO_{t}$ will never be stationary (at most trend-stationary).

**Remind yourself why this is the case** -> The log transformation "linearizes" the process.

**Perform a union of rejection test on $\Delta lnIO_{t}$.**

```{R}

# Union of rejection test 

union_dlnIO <- boot_union(dlnIO)
union_dlnIO
```

**What is the null hypothesis? What is the alternative hypothesis?**

The null is that this times series process has a unit root and the alternative hypothesis is that the times series process is stationary.

**Interpretation**

The p-value is $0$, thus the null hypothesis can be rejected and the process is stationary.

**After having ran the unit root test on $lnIO_{t}$ and $\Delta lnIO_{t}$ what do you conclude about the order of integration of $lnIO_{t}$**

**Explain the difference between a series that is $I(1)$ (“integrated of order one”) or $I(0)$ (“integrated of order zero) in your answer!**

The order of integration of $lnIO_{t}$ is $1$ because we have taken the first order difference of the variable. A series with an order of integration of 0 would be the original $lnIO_{t}$ variable before we applied the difference on it.

**Repeat the exercise for $\Delta lnYO_{t}$**

**Perform a union of rejection test on $\Delta lnYO_{t}$.**

```{R}

# Union of rejection test

union_dlnYO <- boot_union(dlnYO)
union_dlnYO
```

**Interpretation**

The p-value is low $8%$ but it is still above the significance level of $5%$ meaning that we cannot reject the null hypothesis.

```{R}

dlnYO_2 <- diff(dlnYO)

union_dlnYO_2 <- boot_union(dlnYO_2)
union_dlnYO_2
```

It is only when we take the second order difference of $lnYO_{t}$ that we get a p-value of $0$ (as it was the case for $lnIO_{t}$).

**Conclusion:** The order of integration for the variable $lnYO_{t}$ is $I(2)$.

## 8. Static Regression for the series in log-levels (revisited) and Spurious Regressions

Reconsider the static regression model for the series in log-levels:

$$
\tag{14}
lnIO_{t} = \beta_{0} + \beta_{1} lnYO_{t} + u_{t}
$$

**Given the outcome of your unit root tests, is the static regression model $(14)$ possibly a spurious regression?**

**Explain what a spurious regression means and what drives this!**

In the unit root test, we found that $lnIO_{t}$ and $lnYO_{t}$ were likely not stationary due to the trend in the sample. Thus this regression will be a spurious regression since much of the relationship captured by that model would be the result of the positive (somewhat) linear trend of both variables over time.

**Definition**

- Spurious regression 

A spurious regression is a model that shows a misleading statistical evidence of
a linear relationship between independent non-stationary variables.

**Is it “safe” to interpret the regression output of model (14)?**

No. The value of the coefficient will be biased or at least misleading.

**Re-inspect the value of the $R^{2}$. Is it spurious? Should we interpret it?**

```{R}

model14 <- lm(lnIO~lnYO)
summary(model14)
```
The model looks like:

$$
\tag{14'}
lnIO_{t} = \underset{(0.29042)}{-2.30465} + \underset{(0.02387)}{1.06149} \space lnYO_{t} + u_{t}
$$
And the $R^{2}$ is $0.9691$ which is obviously mainly due to the upward linear trend found in both variables.

**What are some solutions to the spurious regression problem?**

The solutions could be to take the differences of these variables (or to add a variable $t$ to detrend the model?).

**Which solutions have we considered already in earlier tutorials, which haven’t we considered yet?**

The difference equation.

## 9. Static Regression for the series in first differences and Spurious Regressions

Reconsider the static regression model for the series in first differences:

$$
\tag{15}
\Delta lnIO_{t} = \beta_{0} + \beta_{1} \Delta lnYO_{t} + u_{t}
$$
**Given the outcome of your unit root tests, is the static regression model $(15)$ possibly a spurious regression?**

It has very low chances since only $\Delta lnYO_{t}$ failed at the 5% significance level (while it passed at the 10%).

**Is it “safe” to interpret the regression output of model (15)?**

It should be much safer than for the previous model.

**Re-inspect the value of the $R^{2}$. Is it spurious? Should we interpret it?**

```{R}

model15 <- lm(dlnIO~dlnYO)
summary(model15)
```

The model looks like:


$$
\tag{15'}
\Delta lnIO_{t} = \underset{(0.01152)}{-0.04765} + \underset{(0.34709)}{2.86299} \space \Delta lnYO_{t} + u_{t}
$$

## 10. ARDL models: Short run and Long run effects

Consider the $ARDL$ model:

$$
\tag{16}
y_{t} = \beta_{0} + \beta_{1} x_{t} + \beta_{2} x_{t-1} + \beta_{3} y_{t-1} + u_{t}
$$

where you may take $y_{t} \equiv lnIO_{t}$ and $x_{t} \equiv lnYO_{t}$.

**Revisit the assumptions needed for OLS to be unbiased or consistent. Can we still rely on strict exogeneity of the regressors in ARDL models? Why (not)?**

The strict exogeneity assumption tells us that the error term in any given period is uncorrelated with the independent variables in all time periods (Assumption TS.3: Zero Conditional Mean).

This cannot hold since the dependent variable $y_{t}$ and the independent variable $x_{t}$ have lagged versions of themselves included in the model.

Contemporanous exogeneity holds -> we have consistency.

**Estimate the ARDL(1,1) model.**

The model we need to estimate is

$$
\tag{16*}
lnIO_{t} = \beta_{0} + \beta_{1} lnYO_{t} + \beta_{2} lnYO_{t - 1} + \beta_{3} lnIO_{t - 1} + u_{t}
$$
```{R}

# Create lagged variables

lags_lnIO <- embed(lnIO, dimension = 2)
lags_lnYO <- embed(lnYO, dimension = 2)

lnIO_0 <- lags_lnIO[, 1]
lnIO_1 <- lags_lnIO[, 2]

lnYO_0 <- lags_lnYO[, 1]
lnYO_1 <- lags_lnYO[, 2]

# Regression

fit_model16 <- lm(lnIO_0~lnYO_0 + lnYO_1 + lnIO_1)
summary(fit_model16)

```

***Short Run* Define the same-year effect, known as the "impact multiplier", as**

$$
\tag{17}
\theta_{1} = \frac{\partial}{\partial x_{t}}E(y_{t}|x_{t}, y_{t-1}, x_{t-1})
$$

The impact multiplier can easily be found with the partial derivative with regards to $x_{t}$. This value is found by looking at $\beta_{1}$.

**What is the value of the impact multiplier for the ARDL(1,1) model you estimated?**

The impact multiplier is $3.35220$.

***Medium run* Define the cumulative effect after two years, known as the “two-year (interim) multiplier”, as**

$$
\begin{equation}
\tag{18}
\begin{aligned}
\theta_{2} &\equiv (\frac {\partial}{\partial x_{t}} + \frac {\partial}{\partial x_{t-1}}) E(y_{t}| x_{t}, y_{t-1}, x_{t-1}, ...) 
\\
&= \theta_{1} + \frac {\partial}{\partial x_{t-1}}E(y_{t}| x_{t}, y_{t-1}, x_{t-1},...)
\end{aligned}
\end{equation}
$$

This is the sum of the impact multiplier and the second-year partial effect of the shock. The two-year multiplier is found as the sum of these two partials,

$$
\theta_{2} \equiv \beta_{2} + \beta_{3}*\beta_{1}
$$

**What is the value of the two-year multiplier for the $ARDL(1,1)$ model you estimated?**

$$
\begin{split}
\theta_{2} &\equiv \beta_{2} + \beta_{3}*\beta_{1} \\
&= -3.09006 + (3.35220*0.78543) \\
&= -0.457141554
\end{split}
$$

The two-year effect is $-0.457$.

***Long-run* Define the cumulative long-run effect, known as the “total multiplier” as**

Using the following equation from the syllabus:

$$
\begin{split}
\theta_{\infty} &= \frac {\beta_{1} + \beta_{2}}{1 - \beta_{3}} \\
&= \frac {3.35220 + (-3.09006)}{1 - 0.78543} \\
&= 1.221699212
\end{split}
$$

The value of the long-run multiplier is $1.22$.

**Obtain the standard error of the estimated impact multiplier, this should be easy. Is the impact multiplier significantly different from zero?**

**Impact multiplier**

For the impact multiplier, since $\theta_{1} = \beta_{1}$, the significance
can be found from the p-value of our regression of model 16. The p-value is $3.34 * 10^{-10}$ which is very close to zero and thus very significant.

**Obtaining the standard error for the two-year and long-run multipliers is more difficult. Let us consider the two-year multiplier.**


```{R}
beta0 = -0.97287 
theta2 = 1
beta2 = -3.09006
beta3 = 0.78543

# Estimate with theta 

nls_theta2 <- nls(lnIO_0 ~ beta0 + ((theta2 - beta2)/(1 + beta3))*lnYO_0 + beta2*lnYO_1 + beta3*lnIO_1, start = list(beta0 = -0.97287, theta2 = 1, beta2 = -3.09006, beta3 = 0.78543))
summary(nls_theta2)

```

```{R}

beta0 = -0.97287 
beta1 = 3.35220
beta2 = -3.09006
beta3 = 0.78543

theta2 = (beta2 + beta1 + (beta3*beta1)) # = 2.895058446

# Estimate with theta 

nls_theta2 <- nls(lnIO_0 ~ beta0 + ((theta2 - beta2)/(1 + beta3))*lnYO_0 + beta2*lnYO_1 + beta3*lnIO_1, start = list(beta0 = -0.97287, theta2 = (beta2 + beta1 + (beta3*beta1)), beta2 = -3.09006, beta3 = 0.78543))
summary(nls_theta2)

```


The coefficient and the standard error of the $\theta_{2}$ is $\underset{(0.29507)}{2.89505}$. The p-value is $4.37*10^{-14}$ which is very significant. Thus the null that $\theta_{2} = 0$ can be rejected.

Version 2:
The coefficient and the standard error of the $\theta_{2}$ is $\underset{(0.29507)}{2.89505}$. The p-value is $4.37*10^{-14}$ which is very significant. Thus the null that $\theta_{2} = 0$ can be rejected.

**Question for tutor**

When *theta2 = 1* in the nls function. The resulting model was the same as when *theta2 = (beta2 + beta1 + (beta3 $\times$ beta1))*. Why?

**Implement a similar *“theta”* trick to get the standard error for $\hat{\theta_{2}}$. Start by re-expressing $\beta_{1}$ in favor of $\theta_{\infty}. Which expression to you get? Run the non-linear regression to get the standard error. Is the long-run multiplier significant?**

$$
\begin{split}
\theta_{\infty} &= \frac {\beta_{1} + \beta_{2}}{1 - \beta_{3}} \\
\theta_{\infty}*(1 - \beta_{3}) &= \beta_{1} + \beta_{2} \\
\beta_{1}  &= \theta_{\infty}*(1 - \beta_{3}) -  \beta_{2}
\end{split}
$$

```{R}
beta0 = -0.97287 
beta1 = 3.35220
beta2 = -3.09006
beta3 = 0.78543

theta_inf = ((beta1 + beta2) / (1 - beta3)) # = 1.221699212

# Estimate the model

nls_theta_inf <- nls(lnIO_0 ~ beta0 + (theta_inf*(1 + beta3)- beta2)*lnYO_0 + beta2*lnYO_1 + beta3*lnIO_1, start = list(beta0 = -0.97287, theta_inf = ((beta1 + beta2) / (1 - beta3)), beta2 = -3.09006, beta3 = 0.78543))
summary(nls_theta_inf)

```
The long-run multiplier is significant with a p-value of $0.000462$.


```{R}


nls_theta_inf <- nls(lnIO_0 ~ beta0 + (theta_inf*(1 + beta3)- beta2)*lnYO_0 + beta2*lnYO_1 + beta3*lnIO_1, start = list(beta0 = 1, theta_inf = 1, beta2 = 1, beta3 = 1))
summary(nls_theta_inf)
```
